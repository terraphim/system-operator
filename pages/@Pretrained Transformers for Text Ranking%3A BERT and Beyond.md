date:: [[Mon, 08.03.2021]]
doi:: 10.1145/3437963.3441667
title:: @Pretrained Transformers for Text Ranking: BERT and Beyond
pages:: 1154-1156
item-type:: [[journalArticle]]
access-date:: 2023-05-08T08:02:55Z
original-title:: Pretrained Transformers for Text Ranking: BERT and Beyond
language:: en
url:: https://dl.acm.org/doi/10.1145/3437963.3441667
short-title:: Pretrained Transformers for Text Ranking
publication-title:: Proceedings of the 14th ACM International Conference on Web Search and Data Mining
authors:: [[Andrew Yates]], [[Rodrigo Nogueira]], [[Jimmy Lin]]
library-catalog:: Semantic Scholar
links:: [Local library](zotero://select/library/items/H4IW4AY3), [Web library](https://www.zotero.org/users/6520516/items/H4IW4AY3)

- [[Abstract]]
	- The goal of text ranking is to generate an ordered list of texts retrieved from a corpus in response to a query for a particular task. Although the most common formulation of text ranking is search, instances of the task can also be found in many text processing applications. This tutorial provides an overview of text ranking with neural network architectures known as transformers, of which BERT (Bidirectional Encoder Representations from Transformers) is the best-known example. These models produce high quality results across many domains, tasks, and settings. This tutorial, which is based on the preprint of a forthcoming book to be published by Morgan and & Claypool under the Synthesis Lectures on Human Language Technologies series, provides an overview of existing work as a single point of entry for practitioners who wish to deploy transformers for text ranking in real-world applications and researchers who wish to pursue work in this area. We cover a wide range of techniques, grouped into two categories: transformer models that perform reranking in multi-stage ranking architectures and learned dense representations that perform ranking directly.
- [[Attachments]]
	- [Semantic Scholar Link](https://www.semanticscholar.org/paper/Pretrained-Transformers-for-Text-Ranking%3A-BERT-and-Lin-Nogueira/2c953a3c378b40dadf2e3fb486713c8608b8e282)